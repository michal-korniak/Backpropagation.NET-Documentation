\input{start-up} 


\begin{document}
\maketitle
\tableofcontents{}
\newpage


\section{Wprowadzenie}
Algorytm wstecznej propagacji błędu jest metodą uczenia wielowarstwowych polegającą na korekcie wag połączeń między neuronami na podstawie błędu całej sieci.
W tym dokumencie prześledzę działanie tego algorytmu,
jednocześnie przedstawiając jego autorską implementację w języku C\#.
Następnie pokaże jak algorytm radzi sobie z nauczaniem funkcji NXoR.

\section{Algorytm wstecznej propagacji}
\subsection{Sposób działania wielowarstwowych sieci neuronowych}
Algorytm wstecznej propagacji jest wykorzystywany do nauczania wielowarstwowych sieci jednokierunkowych.
Taka sieć zawiera warstwę wejściową i warstwę wyjściową,
może posiadać również warstwy ukryte (w zależności od problemu jedną albo dwie).
Każda warstwa zawiera dowolną ilość neuronów.
Neurony są połączone ze sobą w taki sposób, że każdy neuron warstwy innej niż wyjściowa jest połączony z każdym neuronem kolejnej warstwy,
a każde połączenie ma określoną wagę.
Dodatkowo możliwe jest połączenie do tego zwanego biasa, czyli połączenia do neuronu, który zawsze przyjmuje wartość 1.
Połączenia wejściowe do neuronu będą wpływać na to jaką będzie miał wartość.

Rysunek 1 przedstawia przykład jednokierunkowej sieci neuronowej opartej o dwa neurony warstwy wejściowej i tyle samo neuronów w warstwach ukrytej i wyjściowej. 
Oprócz połączeń do innych neuronów, istnieją również połączenia do biasa, który jest wspólny dla neuronów w ramach jednej warstwy.

\begin{figure}[!ht]
    \includegraphics[width=\linewidth]{images/feed-forward-diagram.png}
    \source{https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/}
    \caption{Przykład wielowarstwowej sieci jednokierunkowej}
\end{figure}

Rysunek 1 zawiera wartości neuronów wejściowych oraz wag, dzięki czemu będziemy mogli pokazać jak wyliczane są wartości kolejnych neuronów.
Robimy to sumując wartości połączeń do tego wektora, a następnie poddając wynik funkcji aktywacji.
Przez wartość połączenia będziemy rozumieli iloczyn wagi i neuronu wejściowego dla połączenia.
Przykładowo dla neuronu h1 wartość przed użyciem funkcji aktywacji będzie równa:
\[
  net_{h1}  = i1*w1+i2*w2+1*b1
\]
Co po podstawieniu wartości da:
\[
  net_{h1} = 0.05*0.15+0.10*0.20+1*0.35=0.3775
\]
Taka wartość jest następnie poddawana działaniu funkcji aktywacji. 
Takie działanie umożliwia normalizowanie wartości neuronów do oczekiwanych przedziałów wartości.
W tym przypadku skorzystamy z funkcji sigmoidalnej, której zakres wartości mieści się w przedziale [0,1]:
\[
    f(x) =  \frac{\mathrm{1} }{\mathrm{1} + e^{-x} } 
\]
Po podstawieniu otrzymanej wcześniej wartości do funkcji dostaniemy wartość neuronu h1:
\[
    out_{h1} 
    = f(net_{h1}) 
    =  \frac{\mathrm{1} }{\mathrm{1} + e^{-net_{h1}} } 
    =  \frac{\mathrm{1} }{\mathrm{1} + e^{-0.3775} } \approx 0.5932
\]
W ten sposób będziemy liczyć wartość kolejnych neuronów aż otrzymamy wynik.

\subsection{Implementacja wielowarstwowej sieci neuronowej}

Teraz przedstawię implementację mechanizmu zaprezentowanego w poprzednim podrozdziale.
Jej podstawą jest klasa odwzorowującej sieć neuronową, w naszym przypadku
taką funkcję pełni klasa NeuralNetwork, która to implementuje interfejs pokazany na listingu \ref{lst:INeuralNetwork}.

\begin{lstlisting}[caption={Interfejs INeuralNetwork}, label={lst:INeuralNetwork}]
  public interface INeuralNetwork
  {
      IEnumerable<IInputNeuron> InputLayer { get; }
      IEnumerable<IHiddenNeuron> HiddenLayer { get; }
      IEnumerable<IOutputNeuron> OutputLayer { get; }
      IErrorFunction ErrorFunction { get; }
      void FillInputNeurons(IEnumerable<double> input);
      IEnumerable<double> CalculateOutput();
  }
\end{lstlisting}

W tym momencie skupmy się na linijkach 3, 4 i 5, które to wskazują na to, że sieć neuronowa zawiera warstwy wejściową, ukrytą i wyjściową.
Jak widzimy każda warstwa posiada inny typ neuronu, co wynika to z tego, że w zależności od warstwy neurony się różnią. Przykładowo:
\begin{itemize}
  \item Neuron warstwy wejściowej nie ma możliwości dodawanie połączeń wejściowych.
  \item Neuron warstwy wyjściowej nie ma możliwości dodawanie połączeń wyjściowych.
  \item Neuron warstwy ukrytej ma możliwość dodawanie obu typów połączeń.
\end{itemize}

Obiekt klasy NeuralNetwork jest tworzony z wykorzystaniem wzorca "Builder".
Dzieje się to w sposób pokazany na listingu \ref{lst:NeuralNetworkBuilder}.

\begin{lstlisting} [caption={Budowanie obiektu NeuralNetwork},label={lst:NeuralNetworkBuilder}]
  var neuralNetworkBuilder = new NeuralNetworkBuilder();
  var network = neuralNetworkBuilder
      .SetNumberOfInputNeurons(2)
      .SetNumberOfOutputNeurons(1)
      .SetActivationFunction(new SigmoidActivationFunction())
      .SetErrorFunction(new MeanSquaredErrorFunction(1))
      .SetNumberOfHiddenNeurons(3)  //opcjonalne
      .AddBiasConnections()         //opcjonalne
      .Build();
\end{lstlisting}

Klasa NeuralNetworkBuilder umożliwia stworzenie sieci neuronowej:
\begin{itemize}
  \item Zawierającą wybraną ilość neuronów warstwy wejściowej.
  \item Zawierającą wybraną ilość neuronów warstwy wyjściowej
  \item Działającą na określonej funkcji aktywacji
  \item Wyliczającej błąd na podstawie wybranej funkcji
  \item Umożliwiającej opcjonalne dodanie warstwy ukrytej
  \item Umożliwiającej opcjonalne dodanie połączeń do biasów
\end{itemize}

Zadaniem Buildera jest stworzenie obiektu klasy NeuralNetwork, który spełni podane wymagania. 
W zależności od tego czy użytkownik będzie potrzebował warstwy ukrytej, Builder utworzy sieć która ma połączenia z warstwą pośrednią lub też bezpośrednie połączenia warstwy wejściowej z wyjściową.
Podobnie jest z wyborem tego czy czy powinny być tworzone biasy czy też nie.
Builder nie posiada żadnej metody dotyczącej wyboru pierwotnych wag połączeń, co wynika z tego, że zgodnie z założeniem algorytmu wstecznej propagacji są one inicjalizowane losowo.

Dla tak stworzonej sieci neuronowej możemy przeprowadzić obliczenia, które zostały przedstawione w poprzednim podrozdziale.
Robimy to w sposób przedstawiony w listingu \ref{lst:CalculateOutput}

\begin{lstlisting} [caption={Budowanie obiektu NeuralNetwork},label={lst:CalculateOutput}]
  network.FillInputNeurons(new double[] { 0, 1 });
  var output = network.CalculateOutput();
\end{lstlisting}

W pierwszej kolejności są wypełniane neurony wejściowe.
Jeśli użytkownik poda liczbę danych wejściowych różniącą się od liczby neuronów warstwy wejściowej zostanie wyrzucony wyjątek.

Wyjście sieci jest liczone w sposób rekurencyjny:
\begin{itemize}
  \item Wyjście każdego neuronu z wyjątkiem neuronów wejściowych jest sumą wyjść połączeń wejściowych poddaną funkcji aktywacji (listing \ref{lst:NeuronOutput}).
  \item Wyjście połączenia jest iloczynem wagi i wyjścia neuronu źródłowego (listing \ref{lst:ConnectionOutput})
\end{itemize}

\begin{lstlisting} [caption={Wyliczanie wyjścia neuronu},label={lst:NeuronOutput}]
  public class OutputNeuron : IOutputNeuron
  {
      //***
      public double NetOutput => _inputConnections.Sum(x => x.Output);
      public double Output => _activationFunction.Invoke(NetOutput);
      //***
  }
\end{lstlisting}


\begin{lstlisting} [caption={Wyliczanie wyjścia połączenia},label={lst:ConnectionOutput}]
  class NeuronConnection: INeuronConnection
  {
      //***
      public double Output => Weight * _source.Output;
      //***
  }
\end{lstlisting}

\subsection{Trening sieci neuronowej}
Wagi połączeń są inicjowane wartościami losowymi, w związku z tym możemy się spodziewać, że pierwotne wyjście sieci będzie znacznie odbiegało od tego czego oczekujemy.
Dlatego też przeprowadzany jest trening sieci, którego celem jest ustawienie wag w ten sposób, żeby wyjście sieci jak najbardziej odpowiadało wartościom oczekiwanym.

Pierwszym krokiem jest podanie sieci danych testowych, które posiadają informację na temat wejścia oraz oczekiwanego wyjścia sieci.
Następnie obliczane jest rzeczywiste wyjście neuronu, które są porównywane z oczekiwanym za pomocą funkcji błędu średniokwadratowego.

\[
    E(O)=1/n*(d-y)^2
\]
gdzie:
\begin{itemize}
  \item E(O) to błąd średniokwadratowy dla konkretnego neuronu wyjściowego
  \item n to liczba neuronów warstwy wyjściowej
  \item d to oczekiwane wyjście neuronu
  \item y to rzeczywiste wyjście neuronu
\end{itemize}

Suma błędów średniokwadratowych wszystkich neuronów wyjściowych jest błędem całkowitym.

\begin{figure}[!ht]
  \includegraphics[width=\linewidth]{images/feed-forward-diagram-with-values.png}
  \source{https://mattmazur.com/2015/03/17/a-step-by-step-backpropagation-example/}
  \caption{Przykład wielowarstwowej sieci jednokierunkowej z podanymi wejściami oraz oczekiwanymi wyjściami}
\end{figure}

Rozważmy sieć z rysunku 2, na którym zostały podane wejścia oraz oczekiwane wyjścia sieci.
Wartości neuronów warstwy wyjściowej będą równe:
\[
    out_{o1} \approx 0.7513
\]
\[
    out_{o2} \approx 0.7729
\]
Mając podane oczekiwane oraz rzeczywiste wyjście możemy obliczyć błąd średniokwadratowy dla obu neuronów:
\[
    E_{o1} = 1/2 * (0.01-0.7513)^2 \approx 0.274762845 
\]

\[
    E_{o2} = 1/2 * (0.99-0.7729)^2 \approx 0.023566205 
\]

Tym samym błąd całkowity wynosi:
\[
    E_{total} = E_{o1}+E_{o2} \approx 0.274762845 + 0.023566205 \approx 0.29832905
\]

Jak widzimy sieć jest daleka do ideału, dlatego będziemy chcieli zmodyfikować wagi tak,
aby jak najbardziej zminimalizować błąd.

Robimy to za pomocą wzoru:
\[
  w_5^{+} = w_5 - \eta * \frac{\partial E_{total}}{\partial w_{5}}
\]

gdzie:






\end{document}

