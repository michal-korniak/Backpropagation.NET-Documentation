Jak widzimy otrzymany wynik różni się od oczekiwanego.
Dlatego też przeprowadzimy trening sieci, którego celem będzie ustawienie wag w ten sposób,
żeby wyjście sieci jak najbardziej odpowiadało wartości oczekiwanej.

Mając obliczone wyjście sieci oraz wartością oczekiwaną możemy obliczyć wartość błędu.
Robimy to przy pomocy funkcji błędu średniokwadratowego.

\[
    E(O)=1/n*(d-o)^2
\]
gdzie:
\begin{itemize}
  \item E(O) to błąd średniokwadratowy dla konkretnego neuronu wyjściowego
  \item n to liczba neuronów warstwy wyjściowej
  \item d to oczekiwane wyjście neuronu
  \item o to rzeczywiste wyjście neuronu
\end{itemize}

Dla sieci z rysunku \ref{fig:architecture} i wejścia (0,1) wartość jedynego neuronu wyjściowego jest równa 0.697. 
Natomiast jego wartość oczekiwana to 1. W związku z tym błąd średniokwadratowy jest równy:

\[
    E_{o} = 1/1 * (1-0.697)^2 \approx 0.092
\]


Naszym celem będzie takie ustawienie wag,
aby jak najbardziej zminimalizować błąd. Robimy to za pomocą wzoru:

\begin{gather}
  \begin{aligned}
  w^{+} = w - \eta * \frac{\partial E_{total}}{\partial w}\\
  \end{aligned}\notag
  \shortintertext{gdzie:}
  \begin{aligned}
    &w^{+} ~\text{jest nową wartością wagi}.\\
    &w ~\text{jest aktualną wartością wagi}.\\
    &\eta ~\text{jest stałą uczenia, czyli wartością, która określa szybkość uczenia sieci}.\\
    &\frac{\partial E_{total}}{\partial w} ~\text{jest pochodną częściową błędu całkowitego sieci do danej wagi.}
  \end{aligned}\notag
\end{gather}

Szczególną uwagę należy poświecić stałej uczenia \texteta.
Jest ona ustawiana w momencie rozpoczynania procesu, przyjmując wartości z zakresu [0,1].
W zależności od wybranej wartości zmiany wag będą postępować gwałtowanie lub też łagodnie. 
Zazwyczaj stała uczenia jest ustawiane na wartości mniejsze bądź równe 0.1.
W naszym przypadku będzie to 0.1.

Kolejną rzeczą wartą uwagi jest pochodna błędu całkowitego do konkretnej wagi.
Liczymy ją z wykorzystaniem reguły łańcuchowej, w przypadku w1o:
\[
  \frac{\partial E_{total}}{\partial w1o} = \frac{\partial E_{total}}{\partial out_{o}} * \frac{\partial out_{o}}{\partial net_{o}} * \frac{\partial net_{o}}{\partial w1o}
\]
Już teraz możemy zauważyć, że wzór na pochodną cząstkową dla w2o będzie bardzo podobny:
\[
  \frac{\partial E_{total}}{\partial w2o} = \frac{\partial E_{total}}{\partial out_{o}} * \frac{\partial out_{o}}{\partial net_{o}} * \frac{\partial net_{o}}{\partial w2o}
\]

Wynika to z tego, że oba połączenie mają taki neuron docelowy.
Aby nie liczyć tego samego dwa razy, możemy wprowadzić oznaczenie dla części wspólnej równania:
\[
  \delta_{o} = \frac{\partial E_{total}}{\partial net_{o}} = \frac{\partial E_{total}}{\partial out_{o}} * \frac{\partial out_{o}}{\partial net_{o}} 
\]   
tym samym:
\[
  \frac{\partial E_{total}}{\partial w1o} = \delta_{o}  * \frac{\partial net_{o}}{\partial w1o}
\]
\[
  \frac{\partial E_{total}}{\partial w2o} = \delta_{o}  * \frac{\partial net_{o}}{\partial w2o}
\]

Spróbujmy teraz uprościć wyrażenie \(\frac{\partial net_{o}}{\partial w1o}\):
\[
  \frac{\partial net_{o}}{\partial w1o} = \frac{\partial (w1o * out_{h1} + w2o * out_{h2})}{\partial w1o}
  = 1 * out_{h1} * w1o^{(1 - 1)} + 0 + 0 = out_{h1}
\]
Jak widzimy \(\frac{\partial net_{o}}{\partial w1o}\) ostatecznie sprowadza się do wartości neuronu źródłowego dla połączenia.
W analogiczny sposób upraszcza się równanie dla \(\frac{\partial net_{o}}{\partial w2o}\), dzięki czemu otrzymujemy:
\[
  \frac{\partial E_{total}}{\partial w1o} = \delta_{o}  * out_{h1}
\]
\[
  \frac{\partial E_{total}}{\partial w2o} = \delta_{o}  * out_{h2}
\]

Ostatnim krokiem potrzebnym do wyliczenia nowych wag jest obliczenie samego współczynnika delty dla \(o_1\) (\(\delta_{o}\)):

\[
  \delta_{o} = \frac{\partial E_{total}}{\partial net_{o}} = \frac{\partial E_{total}}{\partial out_{o}} * \frac{\partial out_{o}}{\partial net_{o}}
\]

\[
  \frac{\partial E_{total}}{\partial out_{o}} = \frac{\partial (d_{o} - out_{o})^{2}}{\partial out_{o}}= -(target_{o} - out_{o})
\]


\begin{gather}
  \begin{aligned}
    &\frac{\partial out_{o}}{\partial net_{o}} =  \frac{\partial (\frac{1}{1+e^{-net_{o}}})}{\partial net_{o}}=({\frac{1}{1+e^{-net_{o}}})}(1 - \frac{1}{1+e^{-net_{o}}})
  \end{aligned}\\
  \shortintertext{gdzie:}
  \begin{aligned}
    \frac{1}{1+e^{-net_{o}}}=out_o
  \end{aligned}
  \shortintertext{a więc:}
  \begin{aligned}
    \frac{\partial out_{o}}{\partial net_{o}} = out_{o}(1 - out_{o})
  \end{aligned}
\end{gather}

W związku z powyższymi równaniami wzór na współczynnik delty dla \(out_o\) wynosi:

\[
  \delta_{o} =\frac{\partial E_{total}}{\partial net_{o}} = \frac{\partial E_{total}}{\partial out_{o}} * \frac{\partial out_{o}}{\partial net_{o}}=-(target_{o} - out_{o}) * out_{o}(1 - out_{o})
\]

Co podstawieniu daje:
\[
  \delta_{o} \approx -0.064
\]

W ten sposób otrzymaliśmy łatwy wzór na obliczenie pochodnych cząstkowej po w1o i w2o
\[
  \frac{\partial E_{total}}{\partial w1o} = \delta_{o}  * out_{h1} \approx  -0.064 * 0.525 \approx -0.0336
\]
\[
  \frac{\partial E_{total}}{\partial w2o} = \delta_{o}  * out_{h2} \approx  -0.064 * 0.690 \approx -0.04416
\]

co umożliwia nam obliczenie nowych wag:
\[
  w1o^{+} = w1o - \eta * \frac{\partial E_{total}}{\partial w1o} = 0.4 - 0.1 * -0.0336 = 0.40336 
\]
\[
  w2o^{+} = w2o - \eta * \frac{\partial E_{total}}{\partial w2o} = 0.9 - 0.1 * -0.04416 = 0.904416
\]

Spróbujmy teraz ustalić jakie wartości powinny przyjąć wagi połączeń pomiędzy warstwą wejściową a warstwą ukrytą.
Stosujemy tutaj ten sam wzór co do połączeń pomiędzy warstwą ukrytą a warstwą wyjściową, tak więc wzór na wagę w11 będzie wyglądał tak:
\[
  w11^{+} = w11 - \eta * \frac{\partial E_{total}}{\partial w11}
\]

Tak jak poprzednio najbardziej problematyczne będzie wyznaczenie pochodnej błędu całkowitego do tej wagi:
\[
  \frac{\partial E_{total}}{\partial w11}=
  \frac{\partial E_{total}}{\partial out_{o1}}*
  \frac{\partial out_{o1}}{\partial net_{o1}}*
  \frac{\partial net_{o1}}{\partial out_{h1}}*
  \frac{\partial out_{h1}}{\partial net_{h1}}*
  \frac{\partial net_{h1}}{\partial w11}
\]
Od razu zauważmy, że pochodna błędu całkowitego do wagi drugiego połączenia (w21) do tego neuronu jest bardzo podobna:
\[
  \frac{\partial E_{total}}{\partial w21}=
  \frac{\partial E_{total}}{\partial out_{o1}}*
  \frac{\partial out_{o1}}{\partial net_{o1}}*
  \frac{\partial net_{o1}}{\partial out_{h1}}*
  \frac{\partial out_{h1}}{\partial net_{h1}}*
  \frac{\partial net_{h1}}{\partial w21}
\]

Podobnie jak w przypadku neuronów wyjściowych oznaczmy część wspólną tych równań jako \(\delta h1\):
\[
  \delta h1=
  \frac{\partial E_{total}}{\partial out_{o1}}*
  \frac{\partial out_{o1}}{\partial net_{o1}}*
  \frac{\partial net_{o1}}{\partial out_{h1}}*
  \frac{\partial out_{h1}}{\partial net_{h1}}
\]

Można jednak zauważyć, że pierwsza część równania to tak naprawdę \(\delta o\), którą mamy już wyliczoną:
\[
  \delta h1=
  \delta o *
  \frac{\partial net_{o1}}{\partial out_{h1}}*
  \frac{\partial out_{h1}}{\partial net_{h1}}*
\]

Obliczmy teraz dwie pozostałe pochodne:
\[
  \frac{\partial net_{o1}}{\partial out_{h1}} = \frac{\partial (w1o * out_{h1} + w2o * out_{h2})}{\partial out_{h1}}=w1o
\]
\[
  \frac{\partial out_{h1}}{\partial net_{h1}} = out_{h1}(1 - out_{h1})
\]

Tak więc ostatecznie wyznacznik delta dla neuronu h1 jest równy:
\[
  \delta h1= \delta o * w1o * out_{h1}(1 - out_{h1}) = -0.064 * 0.4 * 0.249375 = -0.006384
\]

Wiedząc, że pochodna wartości neuronu przed funkcją aktywacji po wadze jest równa wartości neuronu z którego wychodzi połączenie
możemy obliczyć również pochodne cząstkowe po wagach w11 i w21
\[
  \frac{\partial E_{total}}{\partial w11}=\delta h1 * \frac{\partial net_{h1}}{\partial w11}=\delta h1 * out_{i1}=-0.006384 * 0=0
\]
\[
  \frac{\partial E_{total}}{\partial w21}=\delta h1 * \frac{\partial net_{h1}}{\partial w21}=\delta h1 * out_{i2}=-0.006384 * 1=-0.006384
\]

Dzięki którym jesteśmy w stanie obliczyć nowe wagi w11 i w21:
\[
  w11^{+} = w11 - \eta * \frac{\partial E_{total}}{\partial w11}=0.5-0.1*0=0.5
\]
\[
  w21^{+} = w21 - \eta * \frac{\partial E_{total}}{\partial w21}=0.1-0.1*-0.006384=0.1006384
\]

W analogiczny sposób wyliczamy również wagi dla połączeń między neuronami warstwy wejściowej a neuronem h2:
\[
  w12^{+} = w12 - \eta * \frac{\partial E_{total}}{\partial w11}=w12 - \eta * \delta h2 * out_{i1}=0.3-0.1*-0.00547584*0=0.3
\]
\[
  w22^{+} = w22 - \eta * \frac{\partial E_{total}}{\partial w21}=w12 - \eta * \delta h2 * out_{i2}=0.8-0.1*-0.00547584*1=0.800547584
\]

Wielokrotne uruchomienie takiego algorytmu sprawi, że sieć będzie dawała rezultaty coraz bardziej zbliżone do tych oczekiwanych.





