
\subsection{Tworzenie obiektu odpowiadającego sieci neuronowej}

Teraz przedstawię autorską implementację przedstawionego wcześniej algorytmu.
Podstawą jest klasa odwzorowującej sieć neuronową, w moim przypadku
taką funkcję pełni klasa NeuralNetwork, która to implementuje interfejs pokazany na listingu \ref{lst:INeuralNetwork}.

\begin{lstlisting}[caption={Interfejs INeuralNetwork}, label={lst:INeuralNetwork}]
  public interface INeuralNetwork
  {
      IEnumerable<IInputNeuron> InputLayer { get; }
      IEnumerable<IHiddenNeuron> HiddenLayer { get; }
      IEnumerable<IOutputNeuron> OutputLayer { get; }
      IErrorFunction ErrorFunction { get; }
      void FillInputNeurons(IEnumerable<double> input);
      IEnumerable<double> CalculateOutput();
  }
\end{lstlisting}

W tym momencie skupmy się na linijkach 3, 4 i 5, które to wskazują na to, że sieć neuronowa zawiera warstwy wejściową, ukrytą i wyjściową.
Jak widzimy każda warstwa posiada inny typ neuronu, co wynika to z tego, że w zależności od warstwy neurony się różnią. Przykładowo:
\begin{itemize}
  \item Neuron warstwy wejściowej nie ma możliwości dodawanie połączeń wejściowych.
  \item Neuron warstwy wyjściowej nie ma możliwości dodawanie połączeń wyjściowych.
  \item Neuron warstwy ukrytej ma możliwość dodawanie obu typów połączeń.
\end{itemize}

Obiekt klasy NeuralNetwork jest tworzony z wykorzystaniem wzorca "Builder".
Dzieje się to w sposób pokazany na listingu \ref{lst:NeuralNetworkBuilder}.

\begin{lstlisting} [caption={Budowanie obiektu NeuralNetwork},label={lst:NeuralNetworkBuilder}]
  var neuralNetworkBuilder = new NeuralNetworkBuilder();
  var network = neuralNetworkBuilder
      .SetNumberOfInputNeurons(2)
      .SetNumberOfOutputNeurons(1)
      .SetActivationFunction(new SigmoidActivationFunction())
      .SetErrorFunction(new MeanSquaredErrorFunction(1))
      .SetNumberOfHiddenNeurons(2)  //opcjonalne
      //.AddBiasConnections()         //opcjonalne
      .Build();
\end{lstlisting}

Klasa NeuralNetworkBuilder umożliwia stworzenie sieci neuronowej:
\begin{itemize}
  \item Zawierającą wybraną ilość neuronów warstwy wejściowej.
  \item Zawierającą wybraną ilość neuronów warstwy wyjściowej
  \item Działającą na określonej funkcji aktywacji
  \item Wyliczającej błąd na podstawie wybranej funkcji
  \item Umożliwiającej opcjonalne dodanie warstwy ukrytej
  \item Umożliwiającej opcjonalne dodanie połączeń do biasów
\end{itemize}

Zadaniem Buildera jest stworzenie obiektu klasy NeuralNetwork, który spełni podane wymagania. 
W zależności od tego czy użytkownik będzie potrzebował warstwy ukrytej, Builder utworzy sieć która ma połączenia z warstwą pośrednią lub też bezpośrednie połączenia warstwy wejściowej z wyjściową.
Podobnie jest z wyborem tego czy czy powinny być tworzone biasy czy też nie.
Builder nie posiada żadnej metody dotyczącej wyboru pierwotnych wag połączeń, co wynika z tego, że zgodnie z założeniem algorytmu wstecznej propagacji są one inicjalizowane losowo.

Dla tak stworzonej sieci neuronowej możemy przeprowadzić obliczenia, które zostały przedstawione w poprzednim podrozdziale.
Robimy to w sposób przedstawiony w listingu \ref{lst:CalculateOutput}

\begin{lstlisting} [caption={Budowanie obiektu NeuralNetwork},label={lst:CalculateOutput}]
  network.FillInputNeurons(new double[] { 0, 1 });
  var output = network.CalculateOutput();
\end{lstlisting}

W pierwszej kolejności są wypełniane neurony wejściowe.
Jeśli użytkownik poda liczbę danych wejściowych różniącą się od liczby neuronów warstwy wejściowej zostanie wyrzucony wyjątek.

Wyjście sieci jest liczone w sposób rekurencyjny:
\begin{itemize}
  \item Wyjście każdego neuronu z wyjątkiem neuronów wejściowych jest sumą wyjść połączeń wejściowych poddanych funkcji aktywacji (listing \ref{lst:NeuronOutput}).
  \item Wyjście połączenia jest iloczynem wagi i wyjścia neuronu źródłowego (listing \ref{lst:ConnectionOutput})
\end{itemize}

\begin{lstlisting} [caption={Wyliczanie wyjścia neuronu},label={lst:NeuronOutput}]
  public class OutputNeuron : IOutputNeuron
  {
      //***
      public double NetOutput => _inputConnections.Sum(x => x.Output);
      public double Output => _activationFunction.Invoke(NetOutput);
      //***
  }
\end{lstlisting}


\begin{lstlisting} [caption={Wyliczanie wyjścia połączenia},label={lst:ConnectionOutput}]
  class NeuronConnection: INeuronConnection
  {
      //***
      public double Output => Weight * _source.Output;
      //***
  }
\end{lstlisting}

\subsection{Implementacja treningu sieci neuronowej}

Aby rozpocząć proces uczenia należy stworzyć instancję klasy Trainer, która to przyjmuje w konstruktorze
informację na temat sieci, którą będziemy chcieli uczyć oraz stałej uczenia. 
Kolejnym krokiem jest przygotowanie danych testowych, na podstawie których nauczymy naszą sieć określonych zachowań.
Następnie należy wywołać funkcję Train, przekazując jej kolekcję danych testowych oraz warunki stopu,
to znaczy maksymalną liczbę epok po których nauczanie się zatrzyma oraz błąd po którego osiągnięciu nauczanie zostanie zakończone.
Cała ta procedura została pokazana na listingu \ref{lst:train-invoke}.


\begin{lstlisting} [caption={Rozpoczęcie procesu uczenia },label={lst:train-invoke}]
  var trainer = new Trainer(neuralNetwork: network, learningRate: 0.01, logger: new ConsoleLogger());
  var trainDataCollection = new[]
  {
      new TrainData(new double []{ 0, 0 },new double [] { 1 } ),
      new TrainData(new double []{ 1, 0 },new double [] { 0 } ),
      new TrainData(new double []{ 0, 1 },new double [] { 0 } ),
      new TrainData(new double []{ 1, 1 },new double [] { 1 } ),
  };
  trainer.Train(trainDataCollection, numberOfEpochs: 1000000, terminalEpochError:0.01);
\end{lstlisting}

Sama funkcja Train steruje przebiegiem uczenia (listing \ref{lst:train-invoke})
W pierwszej kolejności wywołuje funkcję odpowiedzialną za sprawdzenie tego czy w danych testowych liczba danych na wejściu oraz na wyjściu zgadza się z liczbą neuronów w odpowiednich warstwach.
Jeśli ten warunek jest spełniony funkcja wywołuje w pętli metodę TrainForSingleEpoch do czasu aż zostanie spełniony którykolwiek warunek stopu.

\begin{lstlisting} [caption={Funkcja Train},label={lst:train-function}]
  public void Train(TrainData[] trainDataCollection, int numberOfEpochs, double terminalEpochError)
  {
    ValidateTrainData(trainDataCollection);

    int i;
    double epochError = -1;
    var stopwatch = new Stopwatch();
    stopwatch.Start();
    for (i = 0; i < numberOfEpochs; ++i)
    {
        epochError = TrainForSingleEpoch(trainDataCollection);
        _logger.Info($"Epoch {i + 1}, error: {epochError}");
        _logger.Trace("___________________");
        if (epochError <= terminalEpochError)
        {
            break;
        }
    }
    stopwatch.Stop();

    return new TrainStats(i, epochError, (double)stopwatch.ElapsedMilliseconds / 1000);
  }
\end{lstlisting}

Funkcja TrainForSingleEpoch (listing \ref{lst:TrainForSingleEpoch-function}) iteruje po danych testowych wewnątrz jednej epoki wywołując na nich szereg funkcji.
Pierwszą z nich jest ta wypełniająca sieć danymi testowymi na wejściu (linia 6). Dzięki tej operacji jesteśmy w stanie obliczyć wyjście sieci co umożliwia skorzystanie z dwóch kolejnych funkcji.
Funkcja HandleOutputLayer jest w stanie obliczyć błąd średniokwadratowy między wartością oczekiwaną a rzeczywistą oraz współczynnik delta.
Współczynnik ten będzie przydatny przy liczeniu nowych wag zgodnie z opisem z poprzedniego rozdziału.
W przypadku funkcji HandleHiddenLayer liczymy wartość delty dla neuronów warstwy ukrytej.


\begin{lstlisting} [caption={Funkcja TrainForSingleEpoch},label={lst:TrainForSingleEpoch-function}]
  private double TrainForSingleEpoch(TrainData[] trainDataCollection)
  {
    double totalEpochError = 0;
    foreach (var trainData in trainDataCollection)
    {
        _logger.Trace("");
        _logger.Trace($"Input: ({string.Join(",", trainData.Inputs)})");
        _network.FillInputNeurons(trainData.Inputs);
        HandleOutputLayer(trainData.ExpectedOutputs);
        _logger.Trace($"Output: ({string.Join(",", _network.OutputLayer.Select(x => x.Output))})");
        _logger.Trace($"Expected: ({string.Join(",", trainData.ExpectedOutputs)})");
        HandleHiddenLayer();
        UpdateWeights();
        double iterationError = _network.OutputLayer.Sum(x => x.Error);
        _logger.Trace($"Iteration error: {iterationError}");
        totalEpochError += iterationError;
    }
    _logger.Trace("");
    return totalEpochError;
  }

  private void HandleOutputLayer(double[] expectedOutputs)
  {
      for (int i = 0; i < expectedOutputs.Length; ++i)
      {
          var outputNeuron = _network.OutputLayer.ElementAt(i);
          var expectedOutput = expectedOutputs[i];

          outputNeuron.CalculateError(_network.ErrorFunction, expectedOutput);
          outputNeuron.CalculateDeltaError(_network.ErrorFunction, expectedOutput);
      }
  }
  private void HandleHiddenLayer()
  {
      foreach (var hiddenNeuron in _network.HiddenLayer)
      {
          hiddenNeuron.CalculateDeltaError();
      }
  }

\end{lstlisting}

Funkcja UpdateWeights iteruje po wszystkich połączeniach wywołując na nich funkcje aktualizujące wagi.
W przypadku połączeń między neuronami wywoływana jest funkcja przedstawiona w listingu \ref{lst:UpdateNeuronConnectionWeight-function}.
Funkcja ta korzysta ze wzoru, który wynika z obliczeń przedstawionym w podrozdziale dotyczącym korygowania wag:
\[
w^+=w-\eta*\frac{\partial E_{total}}{\partial w}  
\]
gdzie:
\[
  \frac{\partial E_{total}}{\partial w}  =\delta_{output}*input
\]

\begin{lstlisting} [caption={Funkcja TrainForSingleEpoch},label={lst:UpdateNeuronConnectionWeight-function}]
private void UpdateNeuronConnectionWeight(INeuronConnection neuronConnection)
{
    var destinationNeuron = neuronConnection.Destination as INeuronWithDeltaError;
    var derivatateOfTotalErrorToWeight = neuronConnection.Input * destinationNeuron.DeltaError;
    neuronConnection.Weight -= _learningRate * derivatateOfTotalErrorToWeight;
}
\end{lstlisting}

\subsection{Badanie mające na celu wyszukanie najlepszej architektury sieci realizującej funkcję NXOR}

Jak pokazuje listing \ref{lst:train-function} po każdym treningu są zwracane informacje na temat statystyk 
procesu nauczania. Są to:
\begin{itemize}
  \item błąd całkowity dla danych testowych w ostatnim treningu epoki
  \item liczba epok wykonanych w ramach treningu
  \item czas potrzebny na wykonanie treningu
\end{itemize}

Dzięki tym informacjom jesteśmy w stanie przeprowadzić test mający na celu wyszukanie najlepszej architektury
sieci neuronowej realizującej funkcję NXOR.
Jak wiadomo sieć sieć realizująca taka funkcję musi mieć dwa neurony wejściowe oraz jeden wyjściowy,
natomiast kwestią którą będziemy chcieli ustalić jest ilość neuronów ukrytych oraz to
czy warto stworzyć połączenia do biasa.

Aby dowiedzieć się jaka architektura będzie najbardziej optymalna przeprowadzono po 100 testów
dla sieci posiadających od 1 do 5 neuronów w warstwie ukrytej, posiadającej lub nie posiadającej połączenia
do biasa (Klasa Runner.ArchitectureTest.UnipolarNXORArchitectureTest).
Każdy test miał ustawione warunki stopu na 100000 epok lub błąd całkowity równy 0.1.
Następnie zostały porównane mediany dla danych konfiguracji. Oto wyniki:

\begin{scriptsize}  
  Number of neurons in hidden layer: 1
  Bias: NO
  Median results:
  Number of epochs: 100000, average error: 1,00020034941792, elapsed time: 6,905 seconds
  
  Number of neurons in hidden layer: 2
  Bias: NO
  Median results:
  Number of epochs: 100000, average error: 0,3239291988340041, elapsed time: 10,706 seconds
  
  Number of neurons in hidden layer: 3
  Bias: NO
  Median results:
  Number of epochs: 64608, average error: 0,09999773905181816, elapsed time: 8,265 seconds
  
  Number of neurons in hidden layer: 4
  Bias: NO
  Median results:
  Number of epochs: 59230, average error: 0,09999703805996166, elapsed time: 8,445 seconds
  
  Number of neurons in hidden layer: 5
  Bias: NO
  Median results:
  Number of epochs: 59027, average error: 0,09999687258536925, elapsed time: 10,545 seconds
  
  Number of neurons in hidden layer: 1
  Bias: YES
  Median results:
  Number of epochs: 100000, average error: 0,6852163896622288, elapsed time: 11,749 seconds
  
  Number of neurons in hidden layer: 2
  Bias: YES
  Median results:
  Number of epochs: 60466, average error: 0,09999632349029491, elapsed time: 10,212 seconds
  
  Number of neurons in hidden layer: 3
  Bias: YES
  Median results:
  Number of epochs: 49232, average error: 0,09999588993519329, elapsed time: 10,477 seconds
  
  Number of neurons in hidden layer: 4
  Bias: YES
  Median results:
  Number of epochs: 47013, average error: 0,09999732020772159, elapsed time: 12,965 seconds
  
  Number of neurons in hidden layer: 5
  Bias: YES
  Median results:
  Number of epochs: 60467, average error: 0,09999979050186819, elapsed time: 15,098 seconds
\end{scriptsize}

Jak widać sieć osiąga oczekiwany błąd całkowity aż w 7 konfiguracjach.
Ten cel został osiągnięty w najmniejszej liczbie epok w przypadku dla 4 neuronów z biasem.
Jednakże pomimo tego ten przypadek jest dość mało wydajny jeśli spojrzymy na czas, 
gdyż jest on prawie o 5 sekund wolniejszy od najszybszego przypadku z 3 neuronami bez biasa.
Wynika to z tego, że każde dodatkowe połączenie wymaga dodatkowych obliczeń,
co sprawia, że powinniśmy ostrożnie zwiększać ich ilość w naszych sieciach.